{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxscript import ir\n",
    "\n",
    "import onnxdoctor\n",
    "from onnxdoctor.diagnostics_providers import OnnxRuntimeCompatibilityLinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto = onnx.load(\"models/feastconv_Opset16.textproto\")\n",
    "proto = onnx.shape_inference.infer_shapes(proto)\n",
    "model = ir.serde.deserialize_model(proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model should be compatible with ONNX Runtime\n",
    "onnxdoctor.diagnose(model, [OnnxRuntimeCompatibilityLinter()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "    ir_version=8,\n",
      "    opset_imports={'': 16},\n",
      "    producer_name=None,\n",
      "    producer_version=None,\n",
      "    domain=None,\n",
      "    model_version=None,\n",
      ">\n",
      "graph(\n",
      "    name=main_graph,\n",
      "    inputs=(\n",
      "        %\"x\"<FLOAT,?>,\n",
      "        %\"y\"<INT64,?>\n",
      "    ),\n",
      "    outputs=(\n",
      "        %\"val_0\"<?,?>\n",
      "    ),\n",
      ") {\n",
      "    0 |  # node_Add_0\n",
      "         %\"val_0\"<?,?> ⬅️ ::Add(%\"x\", %\"y\")\n",
      "    return %\"val_0\"<?,?>\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Now build an invalid model\n",
    "\n",
    "x = ir.Input(name=\"x\", type=ir.TensorType(ir.DataType.FLOAT))\n",
    "y = ir.Input(name=\"y\", type=ir.TensorType(ir.DataType.INT64))\n",
    "node = ir.Node(\"\", \"Add\", (x, y))\n",
    "graph = ir.Graph([x, y], node.outputs, nodes=[node], name=\"main_graph\", opset_imports={\"\": 16})\n",
    "model2 = ir.Model(graph, ir_version=8)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DiagnosticsMessage(target_type='node', target=Node(name='node_Add_0', domain='', op_type='Add', inputs=(Input('x', type=Tensor(FLOAT), shape=None, producer=None, index=None), Input('y', type=Tensor(INT64), shape=None, producer=None, index=None)), attributes=OrderedDict(), overload='', outputs=(Value('val_0', type=None, shape=None, producer=node_Add_0, index=0),), version=None, doc_string=None), message='ONNX Runtime expects input y of operator ::Add to have type T=FLOAT (bounded by index 0), but found INT64.', severity='error', producer='OnnxRuntimeCompatibilityLinter', error_code='node-type-inconsistent')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnxdoctor.diagnose(model2, [OnnxRuntimeCompatibilityLinter(execution_provider=\"CUDAExecutionProvider\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "    ir_version=8,\n",
      "    opset_imports={'': 21},\n",
      "    producer_name=None,\n",
      "    producer_version=None,\n",
      "    domain=None,\n",
      "    model_version=None,\n",
      ">\n",
      "graph(\n",
      "    name=main_graph,\n",
      "    inputs=(\n",
      "        %\"x\"<FLOAT,?>,\n",
      "        %\"y\"<INT64,?>\n",
      "    ),\n",
      "    outputs=(\n",
      "        %\"val_0\"<?,?>\n",
      "    ),\n",
      ") {\n",
      "    0 |  # node_GroupNormalization_0\n",
      "         %\"val_0\"<?,?> ⬅️ ::GroupNormalization(%\"x\", %\"y\", %\"scale\") {group=1, epsilon=1e-05}\n",
      "    return %\"val_0\"<?,?>\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Now build a model with a new opset\n",
    "\n",
    "x = ir.Input(name=\"x\", type=ir.TensorType(ir.DataType.FLOAT))\n",
    "bias = ir.Input(name=\"y\", type=ir.TensorType(ir.DataType.FLOAT))\n",
    "scale = ir.Input(name=\"scale\", type=ir.TensorType(ir.DataType.FLOAT))\n",
    "node = ir.Node(\"\", \"GroupNormalization\", (x, bias, scale), (ir.AttrInt64(\"group\", 1), ir.AttrFloat32(\"epsilon\", 1e-5)))\n",
    "graph = ir.Graph([x, y], node.outputs, nodes=[node], name=\"main_graph\", opset_imports={\"\": 21})\n",
    "model3 = ir.Model(graph, ir_version=8)\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DiagnosticsMessage(target_type='node', target=Node(name='node_GroupNormalization_0', domain='', op_type='GroupNormalization', inputs=(Input('x', type=Tensor(FLOAT), shape=None, producer=None, index=None), Input('y', type=Tensor(FLOAT), shape=None, producer=None, index=None), Input('scale', type=Tensor(FLOAT), shape=None, producer=None, index=None)), attributes=OrderedDict({'group': AttrInt64('group', 1), 'epsilon': AttrFloat32('epsilon', 1e-05)}), overload='', outputs=(Value('val_0', type=None, shape=None, producer=node_GroupNormalization_0, index=0),), version=None, doc_string=None), message='Operator ::GroupNormalization not supported by CUDAExecutionProvider in ONNX Runtime.', severity='error', producer='OnnxRuntimeCompatibilityLinter', error_code='operator-unsupported')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnxdoctor.diagnose(model3, [OnnxRuntimeCompatibilityLinter(execution_provider=\"CUDAExecutionProvider\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "    ir_version=8,\n",
      "    opset_imports={'': 18, 'com.microsoft': 1},\n",
      "    producer_name=None,\n",
      "    producer_version=None,\n",
      "    domain=None,\n",
      "    model_version=None,\n",
      ">\n",
      "graph(\n",
      "    name=main_graph,\n",
      "    inputs=(\n",
      "        %\"x\"<FLOAT,?>,\n",
      "        %\"y\"<INT64,?>\n",
      "    ),\n",
      "    outputs=(\n",
      "        %\"val_0\"<?,?>\n",
      "    ),\n",
      ") {\n",
      "    0 |  # node_QuickGelu_0\n",
      "         %\"val_0\"<?,?> ⬅️ com.microsoft::QuickGelu(%\"x\")\n",
      "    return %\"val_0\"<?,?>\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# But it does support com.microsoft::QuickGelu\n",
    "\n",
    "x = ir.Input(name=\"x\", type=ir.TensorType(ir.DataType.FLOAT))\n",
    "node = ir.Node(\"com.microsoft\", \"QuickGelu\", (x,))\n",
    "graph = ir.Graph([x, y], node.outputs, nodes=[node], name=\"main_graph\", opset_imports={\"\": 18, \"com.microsoft\": 1})\n",
    "model4 = ir.Model(graph, ir_version=8)\n",
    "print(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnxdoctor.diagnose(model4, [OnnxRuntimeCompatibilityLinter()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
